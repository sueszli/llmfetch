Core flow:

- User enters a URL and defines what data they want to extract (schema)
- System fetches the page content
- LLM processes the raw data and extracts structured data matching the schema
- Results are displayed in a clean, usable format

Features:

- Web server that supports CRUD operations for create/update/delete/list of scrapers.
- Scraping engine that fetches HTML from a given website and processes it with an LLM to extract structured data.
- Suitable database schema for storing the data from the scrapers with the given schema + the status and ID of the scraper all at once (e.g SQLite + some very ergonomic ORM) --> implement this first and also use db.e2e.test.ts to make sure everything works as expected
- Next.js + React for the frontend ----> skip this for now

make sure to have high code locality, use elegant and simple functions and keep things short and simple, just like you would in haskell / C. inline functions that are too short.